# ğŸ‰ SystÃ¨me de Stockage de DonnÃ©es - Installation TerminÃ©e !

## âœ… Ce qui a Ã©tÃ© crÃ©Ã©

### 1. Structure des dossiers
```
data/
â”œâ”€â”€ README.md                 # Documentation complÃ¨te
â”œâ”€â”€ .gitignore               # Ignore les fichiers de donnÃ©es
â”œâ”€â”€ scraped_posts.db         # Base SQLite (auto-crÃ©Ã©e)
â”œâ”€â”€ scraped_posts.jsonl      # Backup JSONL (auto-crÃ©Ã©)
â””â”€â”€ exports/                 # Dossier des exports
    â”œâ”€â”€ .gitkeep
    â””â”€â”€ *.csv, *.json (auto-gÃ©nÃ©rÃ©s)
```

### 2. FonctionnalitÃ©s ajoutÃ©es dans `app/storage.py`

âœ¨ **Nouvelles fonctions** :
- `get_all_posts()` - RÃ©cupÃ¨re tous les posts avec filtres
- `export_to_csv()` - Exporte en CSV
- `export_to_json()` - Exporte en JSON
- `get_stats()` - Statistiques globales

### 3. API FastAPI (`app/main.py`)

ğŸ”„ **Sauvegarde automatique** sur tous les endpoints de scraping :
- `POST /scrape` â†’ âœ… Sauvegarde auto
- `POST /scrape/both` â†’ âœ… Sauvegarde auto (Reddit + StockTwits)

ğŸ“Š **Nouveaux endpoints** :
- `GET /storage/stats` - Statistiques
- `GET /storage/posts` - Consulter les donnÃ©es
- `GET /storage/export/csv` - Export CSV
- `GET /storage/export/json` - Export JSON

### 4. Dashboard Streamlit (`streamlit_app.py`)

ğŸ”„ **Sauvegarde automatique** lors du scraping
ğŸ“Š **Nouvel onglet** : "ğŸ“Š DonnÃ©es StockÃ©es"
- Visualisation graphique
- Filtres par source/mÃ©thode
- Tableau interactif
- Boutons d'export

### 5. Script de test (`test_storage.py`)

Script pour vÃ©rifier que tout fonctionne :
```bash
python test_storage.py
```

## ğŸš€ Comment utiliser

### Via l'API FastAPI (http://127.0.0.1:8000)

**Documentation interactive** : http://127.0.0.1:8000/docs

**Exemples de requÃªtes** :

```bash
# Scraper et sauvegarder automatiquement
curl -X POST "http://127.0.0.1:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"source":"reddit","symbol":"Bitcoin","limit":50}'

# Voir les statistiques
curl http://127.0.0.1:8000/storage/stats

# RÃ©cupÃ©rer les posts Reddit
curl "http://127.0.0.1:8000/storage/posts?source=reddit&limit=100"

# Exporter en CSV
curl "http://127.0.0.1:8000/storage/export/csv?source=reddit"
```

### Via le Dashboard Streamlit

```bash
streamlit run streamlit_app.py
```

Puis :
1. Scraper vos donnÃ©es (automatiquement sauvegardÃ©es !)
2. Aller dans l'onglet "ğŸ“Š DonnÃ©es StockÃ©es"
3. Consulter, filtrer, exporter

### Via Python directement

```python
from app.storage import save_posts, get_all_posts, export_to_csv, get_stats

# RÃ©cupÃ©rer toutes les donnÃ©es Reddit
posts = get_all_posts(source="reddit", limit=100)

# Statistiques
stats = get_stats()
print(f"Total: {stats['total_posts']} posts")

# Export
export_to_csv(source="reddit", method="http")
```

### Via SQLite

```bash
sqlite3 data/scraped_posts.db

SELECT source, method, COUNT(*) 
FROM posts 
GROUP BY source, method;
```

## ğŸ“ Localisation des fichiers

- **Base de donnÃ©es** : `data/scraped_posts.db`
- **Backup JSONL** : `data/scraped_posts.jsonl`
- **Exports** : `data/exports/`

## ğŸ¯ Avantages

âœ… **Automatique** - Aucune action manuelle requise
âœ… **Permanent** - Toutes vos donnÃ©es sont sauvegardÃ©es
âœ… **Flexible** - Multiples formats d'export (SQLite, JSONL, CSV, JSON)
âœ… **DÃ©duplication** - Pas de doublons grÃ¢ce aux UIDs uniques
âœ… **TraÃ§abilitÃ©** - Horodatage de chaque scrape
âœ… **Accessible** - Via API, Dashboard ou Python

## ğŸ“Š Cas d'usage

- **Analyse historique** du sentiment crypto
- **EntraÃ®nement** de modÃ¨les ML
- **Recherche** acadÃ©mique
- **Export** vers Excel, Tableau, etc.
- **Backup** automatique de vos scrapes

## âš¡ Performance

- **SQLite** : Rapide, lÃ©ger, sans serveur
- **JSONL** : Backup ligne par ligne (survie aux crashes)
- **DÃ©duplication** : Index sur UID (clÃ© primaire)

## ğŸ”§ Maintenance

Les fichiers `.db` et `.jsonl` grandissent avec le temps. Pour nettoyer :

```bash
# Sauvegarder puis supprimer
mv data/scraped_posts.db data/backup_$(date +%Y%m%d).db
mv data/scraped_posts.jsonl data/backup_$(date +%Y%m%d).jsonl
```

La base se recrÃ©era automatiquement au prochain scrape.

---

**PrÃªt Ã  utiliser !** ğŸ‰ Toutes vos donnÃ©es seront automatiquement sauvegardÃ©es.
