# ğŸ“Š DonnÃ©es StockÃ©es

Ce dossier contient toutes les donnÃ©es scrapÃ©es depuis Reddit et StockTwits.

## ğŸ“ Structure

```
data/
â”œâ”€â”€ scraped_posts.db      # Base de donnÃ©es SQLite (stockage principal)
â”œâ”€â”€ scraped_posts.jsonl   # Fichier JSONL (backup ligne par ligne)
â””â”€â”€ exports/              # Exports CSV et JSON
    â”œâ”€â”€ scrapes_reddit_20260115_143022.csv
    â”œâ”€â”€ scrapes_stocktwits_20260115_150033.json
    â””â”€â”€ ...
```

## ğŸ’¾ Stockage Automatique

Toutes les donnÃ©es scrapÃ©es sont **automatiquement sauvegardÃ©es** :

### Via l'API FastAPI
- `POST /scrape` â†’ Sauvegarde automatique
- `POST /scrape/both` â†’ Sauvegarde automatique Reddit + StockTwits

### Via le Dashboard Streamlit
- Toute action de scraping â†’ Sauvegarde automatique

## ğŸ“ˆ Consulter les DonnÃ©es

### 1. Via l'API FastAPI

**Statistiques globales :**
```bash
curl http://127.0.0.1:8000/storage/stats
```

**RÃ©cupÃ©rer les posts :**
```bash
# Tous les posts
curl http://127.0.0.1:8000/storage/posts?limit=100

# Filtrer par source
curl http://127.0.0.1:8000/storage/posts?source=reddit&limit=50

# Filtrer par mÃ©thode
curl http://127.0.0.1:8000/storage/posts?method=http&limit=50
```

**Exporter en CSV :**
```bash
curl http://127.0.0.1:8000/storage/export/csv?source=reddit
```

**Exporter en JSON :**
```bash
curl http://127.0.0.1:8000/storage/export/json?source=stocktwits
```

### 2. Via le Dashboard Streamlit

Rendez-vous sur l'onglet **"ğŸ“Š DonnÃ©es StockÃ©es"** :
- Visualisation graphique de la rÃ©partition
- Filtres par source/mÃ©thode
- Tableau interactif
- Boutons d'export CSV/JSON

### 3. Via Python

```python
from app.storage import get_all_posts, export_to_csv, get_stats

# RÃ©cupÃ©rer les posts
posts = get_all_posts(source="reddit", method="http", limit=100)

# Statistiques
stats = get_stats()
print(f"Total: {stats['total_posts']} posts")

# Export
csv_path = export_to_csv(source="reddit")
print(f"ExportÃ© vers: {csv_path}")
```

### 4. Via SQLite directement

```bash
sqlite3 data/scraped_posts.db

# Voir toutes les tables
.tables

# Compter les posts par source
SELECT source, method, COUNT(*) FROM posts GROUP BY source, method;

# Voir les derniers posts
SELECT title, source, scraped_at FROM posts ORDER BY scraped_at DESC LIMIT 10;
```

## ğŸ“Š Structure de la Base de DonnÃ©es

### Table `posts`

| Colonne | Type | Description |
|---------|------|-------------|
| uid | TEXT | ID unique (hash SHA1) |
| id | TEXT | ID original du post |
| source | TEXT | reddit / stocktwits |
| method | TEXT | http / selenium |
| title | TEXT | Titre du post |
| text | TEXT | Contenu du post |
| score | INTEGER | Score/upvotes |
| created_utc | TEXT | Date de crÃ©ation |
| human_label | TEXT | Label humain (Bullish/Bearish pour StockTwits) |
| author | TEXT | Auteur |
| subreddit | TEXT | Subreddit (Reddit uniquement) |
| url | TEXT | URL du post |
| num_comments | INTEGER | Nombre de commentaires |
| scraped_at | TEXT | Date du scraping |

## ğŸ”„ Exports

Les exports sont gÃ©nÃ©rÃ©s dans `data/exports/` avec un timestamp :
- Format CSV : `scrapes_{source}_{method}_{timestamp}.csv`
- Format JSON : `scrapes_{source}_{method}_{timestamp}.json`

Exemples :
- `scrapes_reddit_http_20260115_143022.csv`
- `scrapes_stocktwits_selenium_20260115_150033.json`

## ğŸš€ Utilisation

Les donnÃ©es sont utiles pour :
- **Analyse historique** du sentiment
- **EntraÃ®nement** de modÃ¨les ML
- **Recherche** acadÃ©mique
- **Visualisations** avancÃ©es
- **Exports** pour d'autres outils (Excel, Tableau, etc.)

## ğŸ”’ DÃ©duplication

Le systÃ¨me Ã©vite les doublons grÃ¢ce au champ `uid` (clÃ© primaire) :
- BasÃ© sur : `source:method:post_id`
- Les posts identiques ne sont pas rÃ©insÃ©rÃ©s

## ğŸ“ Notes

- Les fichiers `.db` et `.jsonl` sont synchronisÃ©s
- Le fichier JSONL sert de backup lisible ligne par ligne
- Les exports sont horodatÃ©s pour traÃ§abilitÃ©
- Toutes les dates sont en UTC
